{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11452206,"sourceType":"datasetVersion","datasetId":7175477}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir dataset\n!mkdir dataset/train\n!mkdir dataset/val\n!mkdir dataset/test\n!mkdir dataset/annotations\n!mkdir dataset/train_labels\n!mkdir dataset/val_labels\n!mkdir dataset/test_labels\n","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:00:26.845999Z","iopub.execute_input":"2025-05-21T11:00:26.846230Z","iopub.status.idle":"2025-05-21T11:00:27.757807Z","shell.execute_reply.started":"2025-05-21T11:00:26.846202Z","shell.execute_reply":"2025-05-21T11:00:27.756844Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimages=os.listdir('/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/images')\nprint(len(images))","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:00:30.400358Z","iopub.execute_input":"2025-05-21T11:00:30.400618Z","iopub.status.idle":"2025-05-21T11:00:30.736684Z","shell.execute_reply.started":"2025-05-21T11:00:30.400595Z","shell.execute_reply":"2025-05-21T11:00:30.735945Z"},"trusted":true},"outputs":[{"name":"stdout","text":"14343\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Используемое устройство: {device}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-04T14:03:36.743712Z","iopub.status.busy":"2025-05-04T14:03:36.743417Z","iopub.status.idle":"2025-05-04T14:03:44.848241Z","shell.execute_reply":"2025-05-04T14:03:44.847582Z","shell.execute_reply.started":"2025-05-04T14:03:36.743692Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Используемое устройство: cuda\n"]}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nimport shutil\nfrom PIL import Image\nimport cv2\nimport os\nimport random\n\n\ndef annotations(mask) :\n    ans=[]\n    num_labels, labels = cv2.connectedComponents(mask.astype(np.uint8))\n    #print(num_labels, labels)\n    for i in range(1, num_labels) :\n        ys, xs = np.where(labels == i)\n        min_x, max_x = xs.min(), xs.max()\n        min_y, max_y = ys.min(), ys.max()\n        class_id=mask[ys[0], xs[0]]\n        #print(class_id, xs[0],ys[0])\n        x_center=(min_x+max_x)/2/512\n        y_center=(min_y+max_y)/2/512\n        width=(max_x-min_x+2)/512\n        height=(max_y-min_y+2)/512\n        ans.append([class_id, x_center, y_center, width, height])\n    return ans\n\nimages=os.listdir('/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/images')\nfor i  in tqdm(range(len(images))) :\n    CASE_IDENTIFIER=str(i).zfill(5)\n    \n\n\n    \n    old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/masks/'+images[i]\n    img = Image.open(old_path)\n    img = np.array(img, dtype=np.uint8)\n    if np.max(img)==2 :\n        for j in range(3) :\n            if j==1 :\n                img1=np.fliplr(img)\n            elif j==2 :\n                img1=np.flipud(img)\n            else :\n                img1=img\n            anns=annotations(img1)\n            new_path='/kaggle/working/dataset/train_labels/'+CASE_IDENTIFIER+'_'+str(j)+'.txt'\n            with open(new_path, \"w\", encoding=\"utf-8\") as f:\n                for ann in anns:\n                    if ann[0]==2 :\n                        f.write(f'{1} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n')\n        \n            old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/images/'+images[i]\n            image = Image.open(old_path)\n            image = np.array(image, dtype=np.uint8)\n            if j==1 :\n                img1=np.fliplr(image)\n            elif j==2 :\n                img1=np.flipud(image)\n            else :\n                img1=image\n            new_path='/kaggle/working/dataset/train/'+CASE_IDENTIFIER+'_'+str(j)+'.png'\n            img1 = Image.fromarray(img1)\n            img1.save(new_path)\n\n\n\n\n    else :\n        if np.max(img)==1 :\n            anns=annotations(img)\n            new_path='/kaggle/working/dataset/train_labels/'+CASE_IDENTIFIER+'.txt'\n            with open(new_path, \"w\", encoding=\"utf-8\") as f:\n                for ann in anns:\n                    if ann[0]!=1 :\n                        f.write(f'{0} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n')\n        \n            old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/images/'+images[i]\n            new_path='/kaggle/working/dataset/train/'+CASE_IDENTIFIER+'.png'\n            shutil.copy(old_path, new_path)\n        elif random.random()<0.2 :\n            anns=annotations(img)\n            new_path='/kaggle/working/dataset/train_labels/'+CASE_IDENTIFIER+'.txt'\n            with open(new_path, \"w\", encoding=\"utf-8\") as f:\n                for ann in anns:\n                    if ann[0]!=1 :\n                        f.write(f'{0} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n')\n                        \n            old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/train/images/'+images[i]\n            new_path='/kaggle/working/dataset/train/'+CASE_IDENTIFIER+'.png'\n            shutil.copy(old_path, new_path)","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:10:34.298554Z","iopub.execute_input":"2025-05-21T11:10:34.299184Z","iopub.status.idle":"2025-05-21T11:17:05.760190Z","shell.execute_reply.started":"2025-05-21T11:10:34.299158Z","shell.execute_reply":"2025-05-21T11:17:05.759433Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 14343/14343 [06:31<00:00, 36.67it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"len(os.listdir('/kaggle/working/dataset/train/'))","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:17:46.130878Z","iopub.execute_input":"2025-05-21T11:17:46.131557Z","iopub.status.idle":"2025-05-21T11:17:46.142019Z","shell.execute_reply.started":"2025-05-21T11:17:46.131530Z","shell.execute_reply":"2025-05-21T11:17:46.141402Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"7896"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\n\n\nimages=os.listdir('/kaggle/input/lidc-idri-preprocessed/CT images and masks/val/images')\nfor i  in tqdm(range(len(images))) :\n    CASE_IDENTIFIER=str(i).zfill(5)\n\n\n    \n    old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/val/masks/'+images[i]\n    img = Image.open(old_path)\n    img = np.array(img, dtype=np.uint8)\n    if np.max(img)<3 :\n        anns=annotations(img)\n        new_path='/kaggle/working/dataset/val_labels/'+CASE_IDENTIFIER+'.txt'\n        with open(new_path, \"w\", encoding=\"utf-8\") as f:\n            for ann in anns:\n                if ann[0]==2 :\n                    f.write(f'{1} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n')\n\n\n        old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/val/images/'+images[i]\n        new_path='/kaggle/working/dataset/val/'+CASE_IDENTIFIER+'.png'\n        shutil.copy(old_path, new_path)","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:17:49.301791Z","iopub.execute_input":"2025-05-21T11:17:49.302532Z","iopub.status.idle":"2025-05-21T11:19:23.622133Z","shell.execute_reply.started":"2025-05-21T11:17:49.302507Z","shell.execute_reply":"2025-05-21T11:19:23.621569Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 3391/3391 [01:34<00:00, 35.99it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nimages=os.listdir('/kaggle/input/lidc-idri-preprocessed/CT images and masks/test/images')\nfor i  in tqdm(range(len(images))) :\n    CASE_IDENTIFIER=str(i).zfill(5)\n\n\n    \n    old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/test/masks/'+images[i]\n    img = Image.open(old_path)\n    img = np.array(img, dtype=np.uint8)\n    if np.max(img)<3 :\n        anns=annotations(img)\n        new_path='/kaggle/working/dataset/test_labels/'+CASE_IDENTIFIER+'.txt'\n        with open(new_path, \"w\", encoding=\"utf-8\") as f:\n            for ann in anns:\n                if ann[0]==2 :\n                    f.write(f'{1} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n')\n\n        \n        old_path = '/kaggle/input/lidc-idri-preprocessed/CT images and masks/test/images/'+images[i]\n        new_path='/kaggle/working/dataset/test/'+CASE_IDENTIFIER+'.png'\n        shutil.copy(old_path, new_path)","metadata":{"execution":{"iopub.status.busy":"2025-05-21T11:20:39.740542Z","iopub.execute_input":"2025-05-21T11:20:39.740787Z","iopub.status.idle":"2025-05-21T11:20:55.111520Z","shell.execute_reply.started":"2025-05-21T11:20:39.740761Z","shell.execute_reply":"2025-05-21T11:20:55.110937Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 3330/3330 [00:15<00:00, 216.76it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:56:25.599200Z","iopub.execute_input":"2025-05-21T11:56:25.599552Z","iopub.status.idle":"2025-05-21T11:56:29.998335Z","shell.execute_reply.started":"2025-05-21T11:56:25.599525Z","shell.execute_reply":"2025-05-21T11:56:29.997518Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class YoloDetectionDataset(Dataset):\n    def __init__(self, img_dir, label_dir, transforms=None, img_ext=\".png\"):\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.transforms = transforms\n        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(img_ext)])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        label_path = os.path.join(self.label_dir, img_name.replace('.png', '.txt'))\n\n        image = Image.open(img_path).convert(\"RGB\")\n        w, h = image.size\n\n        boxes = []\n        labels = []\n\n        if os.path.exists(label_path):\n            with open(label_path, \"r\") as f:\n                for line in f:\n                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n                    # YOLO → COCO\n                    xmin = (x_center - width / 2) * w\n                    xmax = (x_center + width / 2) * w\n                    ymin = (y_center - height / 2) * h\n                    ymax = (y_center + height / 2) * h\n                    boxes.append([xmin, ymin, xmax, ymax])\n                    labels.append(1)  # malignant class\n\n        if boxes:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n        else:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx])\n        }\n\n        if self.transforms:\n            image = self.transforms(image)\n\n        return image, target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:01:14.503235Z","iopub.execute_input":"2025-05-21T12:01:14.503512Z","iopub.status.idle":"2025-05-21T12:01:14.513118Z","shell.execute_reply.started":"2025-05-21T12:01:14.503492Z","shell.execute_reply":"2025-05-21T12:01:14.512441Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ntrain_dataset = YoloDetectionDataset(\n    img_dir=\"/kaggle/working/dataset/train\",\n    label_dir=\"/kaggle/working/dataset/train_labels\",\n    transforms=transform\n)\n\nval_dataset = YoloDetectionDataset(\n    img_dir=\"/kaggle/working/dataset/val\",\n    label_dir=\"/kaggle/working/dataset/val_labels\",\n    transforms=transform\n)\n\ntest_dataset = YoloDetectionDataset(\n    img_dir=\"/kaggle/working/dataset/test\",\n    label_dir=\"/kaggle/working/dataset/test_labels\",\n    transforms=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:01:16.360691Z","iopub.execute_input":"2025-05-21T12:01:16.361412Z","iopub.status.idle":"2025-05-21T12:01:16.379919Z","shell.execute_reply.started":"2025-05-21T12:01:16.361387Z","shell.execute_reply":"2025-05-21T12:01:16.379259Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:58:53.383032Z","iopub.execute_input":"2025-05-21T11:58:53.383313Z","iopub.status.idle":"2025-05-21T11:58:55.196603Z","shell.execute_reply.started":"2025-05-21T11:58:53.383291Z","shell.execute_reply":"2025-05-21T11:58:55.195969Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:00<00:00, 185MB/s]  \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport time\nimport copy\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Оптимизатор\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:59:09.385617Z","iopub.execute_input":"2025-05-21T11:59:09.386400Z","iopub.status.idle":"2025-05-21T11:59:09.678892Z","shell.execute_reply.started":"2025-05-21T11:59:09.386373Z","shell.execute_reply":"2025-05-21T11:59:09.678120Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    total_loss = 0\n    for images, targets in data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        total_loss += losses.item()\n    return total_loss / len(data_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:00:08.790300Z","iopub.execute_input":"2025-05-21T12:00:08.790892Z","iopub.status.idle":"2025-05-21T12:00:08.796318Z","shell.execute_reply.started":"2025-05-21T12:00:08.790868Z","shell.execute_reply":"2025-05-21T12:00:08.795441Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"num_epochs = 30\nbest_model_wts = copy.deepcopy(model.state_dict())\nbest_loss = float(\"inf\")\n\nfor epoch in range(num_epochs):\n    loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n    lr_scheduler.step()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss:.4f}\")\n\n    if loss < best_loss:\n        best_loss = loss\n        best_model_wts = copy.deepcopy(model.state_dict())\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(\"Saved best model.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T12:01:22.223160Z","iopub.execute_input":"2025-05-21T12:01:22.223642Z","iopub.status.idle":"2025-05-21T13:20:20.373039Z","shell.execute_reply.started":"2025-05-21T12:01:22.223620Z","shell.execute_reply":"2025-05-21T13:20:20.372146Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30, Training Loss: 0.0889\nSaved best model.\nEpoch 2/30, Training Loss: 0.0611\nSaved best model.\nEpoch 3/30, Training Loss: 0.0506\nSaved best model.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3789082701.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/876221076.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"model.load_state_dict(best_model_wts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:20:23.166812Z","iopub.execute_input":"2025-05-21T13:20:23.167071Z","iopub.status.idle":"2025-05-21T13:20:23.181307Z","shell.execute_reply.started":"2025-05-21T13:20:23.167052Z","shell.execute_reply":"2025-05-21T13:20:23.180650Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model.eval()\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:20:27.840650Z","iopub.execute_input":"2025-05-21T13:20:27.841298Z","iopub.status.idle":"2025-05-21T13:20:27.851064Z","shell.execute_reply.started":"2025-05-21T13:20:27.841261Z","shell.execute_reply":"2025-05-21T13:20:27.850464Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:20:36.513068Z","iopub.execute_input":"2025-05-21T13:20:36.513702Z","iopub.status.idle":"2025-05-21T13:20:36.518061Z","shell.execute_reply.started":"2025-05-21T13:20:36.513673Z","shell.execute_reply":"2025-05-21T13:20:36.517287Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"all_preds = []\nall_targets = []\n\nwith torch.no_grad():\n    for images, targets in test_loader:\n        images = [img.to(device) for img in images]\n        outputs = model(images)\n\n        for output, target in zip(outputs, targets):\n            pred_boxes = output['boxes'].cpu()\n            pred_scores = output['scores'].cpu()\n            pred_labels = output['labels'].cpu()\n\n            true_boxes = target['boxes'].cpu()\n            true_labels = target['labels'].cpu()\n\n            all_preds.append({\n                'boxes': pred_boxes,\n                'scores': pred_scores,\n                'labels': pred_labels,\n            })\n\n            all_targets.append({\n                'boxes': true_boxes,\n                'labels': true_labels,\n            })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:20:50.716530Z","iopub.execute_input":"2025-05-21T13:20:50.716859Z","iopub.status.idle":"2025-05-21T13:25:33.882283Z","shell.execute_reply.started":"2025-05-21T13:20:50.716837Z","shell.execute_reply":"2025-05-21T13:25:33.881758Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"pip install torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:25:49.923312Z","iopub.execute_input":"2025-05-21T13:25:49.923580Z","iopub.status.idle":"2025-05-21T13:25:52.962691Z","shell.execute_reply.started":"2025-05-21T13:25:49.923560Z","shell.execute_reply":"2025-05-21T13:25:52.961766Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"from torchmetrics.detection.mean_ap import MeanAveragePrecision\nmetric = MeanAveragePrecision()\nmetric.update(all_preds, all_targets)\nresults = metric.compute()\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:26:00.615555Z","iopub.execute_input":"2025-05-21T13:26:00.615885Z","iopub.status.idle":"2025-05-21T13:26:05.796310Z","shell.execute_reply.started":"2025-05-21T13:26:00.615859Z","shell.execute_reply":"2025-05-21T13:26:05.795614Z"}},"outputs":[{"name":"stdout","text":"{'map': tensor(0.3775), 'map_50': tensor(0.6087), 'map_75': tensor(0.4332), 'map_small': tensor(0.3654), 'map_medium': tensor(0.5666), 'map_large': tensor(-1.), 'mar_1': tensor(0.5262), 'mar_10': tensor(0.5741), 'mar_100': tensor(0.5741), 'mar_small': tensor(0.5637), 'mar_medium': tensor(0.7211), 'mar_large': tensor(-1.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(1, dtype=torch.int32)}\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import torch\n\ndef box_iou(boxes1, boxes2):\n\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N, M, 2]\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N, M, 2]\n\n    wh = (rb - lt).clamp(min=0)  # [N, M, 2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N, M]\n\n    union = area1[:, None] + area2 - inter\n    iou = inter / union\n    return iou\n\ndef calculate_precision_recall(all_preds, all_targets, iou_threshold=0.5, score_threshold=0.5):\n    TP, FP, FN = 0, 0, 0\n\n    for preds, targets in zip(all_preds, all_targets):\n        pred_boxes = preds['boxes']\n        pred_scores = preds['scores']\n        pred_boxes = pred_boxes[pred_scores > score_threshold]\n        gt_boxes = targets['boxes']\n\n        if len(pred_boxes) == 0:\n            FN += len(gt_boxes)\n            continue\n        if len(gt_boxes) == 0:\n            FP += len(pred_boxes)\n            continue\n\n        ious = box_iou(pred_boxes, gt_boxes)\n        matched_gt = set()\n\n        for i in range(len(pred_boxes)):\n            max_iou, idx = torch.max(ious[i], dim=0)\n            if max_iou >= iou_threshold and idx.item() not in matched_gt:\n                TP += 1\n                matched_gt.add(idx.item())\n            else:\n                FP += 1\n\n        FN += len(gt_boxes) - len(matched_gt)\n\n    precision = TP / (TP + FP + 1e-6)\n    recall = TP / (TP + FN + 1e-6)\n\n    return precision, recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:31:00.351329Z","iopub.execute_input":"2025-05-21T13:31:00.351602Z","iopub.status.idle":"2025-05-21T13:31:00.360134Z","shell.execute_reply.started":"2025-05-21T13:31:00.351582Z","shell.execute_reply":"2025-05-21T13:31:00.359311Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"precision, recall = calculate_precision_recall(all_preds, all_targets, iou_threshold=0.5, score_threshold=0.5)\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:31:01.934103Z","iopub.execute_input":"2025-05-21T13:31:01.934579Z","iopub.status.idle":"2025-05-21T13:31:02.051305Z","shell.execute_reply.started":"2025-05-21T13:31:01.934555Z","shell.execute_reply":"2025-05-21T13:31:02.050763Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.1475, Recall: 0.8042\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"precision, recall = calculate_precision_recall(all_preds, all_targets, iou_threshold=0.5, score_threshold=0.9)\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:31:29.343007Z","iopub.execute_input":"2025-05-21T13:31:29.343581Z","iopub.status.idle":"2025-05-21T13:31:29.447137Z","shell.execute_reply.started":"2025-05-21T13:31:29.343560Z","shell.execute_reply":"2025-05-21T13:31:29.446525Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.3883, Recall: 0.7413\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"precision, recall = calculate_precision_recall(all_preds, all_targets, iou_threshold=0.5, score_threshold=0.95)\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:31:38.700655Z","iopub.execute_input":"2025-05-21T13:31:38.701409Z","iopub.status.idle":"2025-05-21T13:31:38.800152Z","shell.execute_reply.started":"2025-05-21T13:31:38.701374Z","shell.execute_reply":"2025-05-21T13:31:38.799602Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.5172, Recall: 0.6853\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"precision, recall = calculate_precision_recall(all_preds, all_targets, iou_threshold=0.5, score_threshold=0.97)\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:31:48.445014Z","iopub.execute_input":"2025-05-21T13:31:48.445554Z","iopub.status.idle":"2025-05-21T13:31:48.540414Z","shell.execute_reply.started":"2025-05-21T13:31:48.445532Z","shell.execute_reply":"2025-05-21T13:31:48.539681Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.6290, Recall: 0.6224\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"precision, recall = calculate_precision_recall(all_preds, all_targets, iou_threshold=0.1, score_threshold=0.97)\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:33:44.111124Z","iopub.execute_input":"2025-05-21T13:33:44.111477Z","iopub.status.idle":"2025-05-21T13:33:44.207390Z","shell.execute_reply.started":"2025-05-21T13:33:44.111452Z","shell.execute_reply":"2025-05-21T13:33:44.206844Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.6360, Recall: 0.6294\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}